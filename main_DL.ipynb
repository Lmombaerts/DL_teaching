{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\", save_best_only=True)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([COUNT_PNEUMONIA / COUNT_NORMAL])\n",
    "print(\"Initial bias: {:.5f}\".format(initial_bias[0]))\n",
    "\n",
    "TRAIN_IMG_COUNT = COUNT_NORMAL + COUNT_PNEUMONIA\n",
    "weight_for_0 = (1 / COUNT_NORMAL) * (TRAIN_IMG_COUNT) / 2.0\n",
    "weight_for_1 = (1 / COUNT_PNEUMONIA) * (TRAIN_IMG_COUNT) / 2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
    "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3bdfc9b1c248>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "#import keras (high level API) wiht tensorflow as backend\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 2 Convolution layer with Max pooling\n",
    "    model.add(Conv1D(filters=8, kernel_size=51, activation = 'relu', padding = 'same', input_shape = input_shape))\n",
    "    model.add(AveragePooling1D(pool_size=10))\n",
    "    model.add(Conv1D(filters=2, kernel_size=9, activation = 'relu', padding = 'same', kernel_initializer = \"he_normal\"))\n",
    "    model.add(AveragePooling1D())  \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # 3 Full connected layer\n",
    "    model.add(Dense(16, activation = 'relu', kernel_initializer = \"he_normal\"))\n",
    "    #model.add(Dense(32, activation = 'relu', kernel_initializer = \"he_normal\"))\n",
    "    #model.add(Dense(54, activation = 'relu', kernel_initializer = \"he_normal\"))\n",
    "    model.add(Dense(2, activation = 'softmax')) # 2 classes \n",
    "    \n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_and_fit_model(cnn_model, x_train, y_train, x_test, y_test):\n",
    "    #history = cnn_model.fit(x_train,y_train, epochs=100, batch_size=64, verbose=2)\n",
    "    #_, accuracy = cnn_model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
    "    history = cnn_model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=250, batch_size=128, verbose=0)\n",
    "    return cnn_model, history\n",
    "\n",
    "input_shape = (x_train.shape[1],1)\n",
    "cnn_model = build_cnn_model(input_shape)\n",
    "\n",
    "# train cnn model\n",
    "x_train_reshaped = np.array(x_train).reshape((x_train.shape[0],x_train.shape[1],1))\n",
    "x_test_reshaped = np.array(x_test).reshape((x_test.shape[0],x_test.shape[1],1))\n",
    "trained_cnn_model, cnn_history = compile_and_fit_model(cnn_model, x_train_reshaped, y_train, x_test_reshaped, y_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cnn_history.history['loss'],label='Loss')\n",
    "plt.plot(cnn_history.history['val_loss'],label='Validation Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(cnn_history.history['accuracy'],label='Accuracy')\n",
    "plt.plot(cnn_history.history['val_accuracy'],label='Validation Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_1D_CNN(trainX, trainy, testX, testy):\n",
    "    epochs = 25 \n",
    "    batch_size = 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features))) \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(n_outputs,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit\n",
    "    history = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=2)\n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_multihead_CNN(trainX, trainy, testX, testy):\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    n_timesteps, n_features, n_outputs = x_train_raw.shape[1], x_train_raw.shape[2], y_train.shape[1]\n",
    "    \n",
    "    # head 1\n",
    "    inputs1 = Input(shape=(n_timesteps, n_features))\n",
    "    conv1 = Conv1D(filters=64, kernel=3, activation='relu')(inputs1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # head 2\n",
    "    inputs2 = Input(shape=(n_timesteps, n_features))\n",
    "    conv2 = Conv1D(filters=64, kernel=3, activation='relu')(inputs2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    \n",
    "    # head 3\n",
    "    inputs3 = Input(shape=(n_timesteps, n_features))\n",
    "    conv3 = Conv1D(filters=64, kernel=3, activation='relu')(inputs3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # merge\n",
    "    merged = concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    # interpretation\n",
    "    dense = Dense(100, activaition='relu')(merged)\n",
    "    outputs = Dense(n_outputs, activation='softmax')(dense)\n",
    "    model = Model(inputs=[inputs1,inputs2,inputs3], outputs=outputs)\n",
    "    \n",
    "    # compile \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit\n",
    "    history = model.fit([trainX,trainX,trainX], trainy, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate([testX,testX,testX], testy, batch_size=batch_size, verbose=2)\n",
    "    plot_model(model, to_file='multihead_CNN_example.png')\n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla LSTM\n",
    "def evaluate_model_LSTM(x_train_raw, y_train, x_test_raw, y_test):\n",
    "    n_timesteps, n_features, n_outputs = x_train_raw.shape[1], x_train_raw.shape[2], y_train.shape[1]\n",
    "    epochs = 25\n",
    "    batch_size = 64\n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation = 'softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit\n",
    "    history = model.fit(x_train_raw,y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    # Evaluation\n",
    "    results, accuracy = model.evaluate(x_test_raw, y_test, batch_size=batch_size, verbose=2)\n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "The CNN LSTM involves CNN layers to extracte the features on input data\n",
    "combined with LSTM to support sequence prediction.\n",
    "\n",
    "The CNN model will read subsequences of the main sequence in as blocks,\n",
    "extract features from each block, then allow the LSTM to interpret the\n",
    "features extracted from each block.\n",
    "\n",
    "One approach to implementing this model is to split each window of time steps\n",
    "into smaller subsequences for the CNN to process. (here 4x32=128)\n",
    "\n",
    "It is common to use two consecutive CNN layers followed by dropout and\n",
    "a maxpooling layer, which is the simple structure used here. Indeed, Conv layers\n",
    "summarize the presence of features in an input image. The problem is that those\n",
    "are sensitive to the location of the features in the input. One approach to \n",
    "address this sensitivity is to down sample the feature maps. This has the \n",
    "effect of making the resulting down sampled feature maps more robust to changes \n",
    "in the position of the feature in the image, referred to by the technical phrase\n",
    "\"local translation invariance\". Pooling layers provide an approach to down\n",
    "sampling feature maps by summarizing the presence of features in patches\n",
    "of the feature map. Two common pooling methods are average pooling and max pooling\n",
    "to summarize the average presence of a feature and the most actived presence\n",
    "of a feature respectively.\n",
    "\n",
    "Kernel_size = 3 (means you lose two datapoints on the next layer)\n",
    "filters = 64 (means the batch size actually)\n",
    "pool_size = 2 (means the size of feature map reduced by a factor of 2, almost always)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_CNN_LSTM(x_train_raw, y_train, x_test_raw, y_test):\n",
    "    _, n_features, n_outputs = x_train_raw.shape[1], x_train_raw.shape[2], y_train.shape[1]\n",
    "    epochs = 50\n",
    "    batch_size = 64\n",
    "    # Reshape in subsequences for the CNN\n",
    "    n_subs, n_lengths = 4, 32\n",
    "    x_train_raw = x_train_raw.reshape(x_train_raw.shape[0], n_subs, n_lengths, n_features)\n",
    "    x_test_raw = x_test_raw.reshape(x_test_raw.shape[0], n_subs, n_lengths, n_features)\n",
    "    # Define CNN LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=batch_size, kernel_size=3, activation='relu'), input_shape=(None,n_lengths,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=batch_size, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit\n",
    "    history = model.fit(x_train_raw, y_train, epochs=epochs, batch_size=batch_size, verbose = 2)\n",
    "    # Evaluate model\n",
    "    _, accuracy = model.evaluate(x_test_raw,y_test, batch_size=batch_size, verbose = 2)\n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "A further extension of the CNN LSTM idea is to perform the convolutions of the \n",
    "CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM.\n",
    "\n",
    "Unlike an LSTM that reads the data in directly in order to calculate internal \n",
    "state and state transitions, and unlike the CNN LSTM that is interpreting the \n",
    "output from CNN models, the ConvLSTM is using convolutions directly as part of \n",
    "reading input into the LSTM units themselves.\n",
    "\n",
    "For this chosen framing of the problem, the input for the ConvLSTM2D would therefore be:\n",
    "reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "\n",
    "Samples: n, for the number of windows in the dataset.\n",
    "Time: 4, for the four subsequences that we split a window of 128 time steps into.\n",
    "Rows: 1, for the one-dimensional shape of each subsequence.\n",
    "Columns: 32, for the 32 time steps in an input subsequence.\n",
    "Channels: 9, for the nine input variables.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_ConvLSTM(x_train_raw, y_train, x_test_raw, y_test):\n",
    "    _, n_features, n_outputs = x_train_raw.shape[1], x_train_raw.shape[2], y_train.shape[1]\n",
    "    epochs = 50\n",
    "    batch_size = 64\n",
    "    # Reshaping of the ConvLSTM\n",
    "    n_steps, n_length = 4, 32\n",
    "    x_train_raw = x_train_raw.reshape((x_train_raw.shape[0], n_steps, 1, n_length, n_features))\n",
    "    x_test_raw = x_test_raw.reshape((x_test_raw.shape[0], n_steps, 1, n_length, n_features))\n",
    "\t# define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "    history = model.fit(x_train_raw, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\t# evaluate model\n",
    "    _, accuracy = model.evaluate(x_test_raw, y_test, batch_size=batch_size, verbose=2)\n",
    "    return history, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, test, raw, scaler, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\t# prepare model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\t# fit model\n",
    "\ttrain_rmse, test_rmse = list(), list()\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\t\t# evaluate model on train data\n",
    "\t\traw_train = raw[-(len(train)+len(test)+1):-len(test)]\n",
    "\t\ttrain_rmse.append(evaluate(model, raw_train, train, scaler, 0, batch_size))\n",
    "\t\tmodel.reset_states()\n",
    "\t\t# evaluate model on test data\n",
    "\t\traw_test = raw[-(len(test)+1):]\n",
    "\t\ttest_rmse.append(evaluate(model, raw_test, test, scaler, 0, batch_size))\n",
    "\t\tmodel.reset_states()\n",
    "\thistory = DataFrame()\n",
    "\thistory['train'], history['test'] = train_rmse, test_rmse\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "def conv_block(filters, inputs):\n",
    "    x = layers.SeparableConv2D(filters, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.SeparableConv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.MaxPool2D()(x)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def dense_block(units, dropout_rate, inputs):\n",
    "    x = layers.Dense(units, activation=\"relu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = preprocessing.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "\n",
    "    x = conv_block(32, x)\n",
    "    x = conv_block(64, x)\n",
    "\n",
    "    x = conv_block(128, x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = conv_block(256, x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = dense_block(512, 0.7, x)\n",
    "    x = dense_block(128, 0.5, x)\n",
    "    x = dense_block(64, 0.3, x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fd1fa361db47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = keras.Sequential(\n\u001b[0;32m      2\u001b[0m     [\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         layers.Conv1D(\n\u001b[0;32m      5\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
